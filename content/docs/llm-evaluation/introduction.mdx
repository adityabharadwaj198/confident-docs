---
title: Introduction
---

import { Callout, Cards } from "nextra/components";

# Introduction to LLM Evaluation on Confident AI

Confident AI's evaluation features are second-to-none, and in fact all features you've seen up to this point in the documentation leads up to the LLM evaluation suite.

<details>

<summary>What does Confident AI's LLM Evaluation offer?</summary>

We offer metrics that are:

- **Default**, battle-tested, open-source, and plug-and-play
- **Custom**, research-backed, and easy to create in natrual language
- **For any use case**, LLM system architecture, or framework

And allow you to run evaluation in both **CI/CD** environments and as a separate Python script. 

<Callout>
You can also run evals on an individual [component-level](/docs/llm-evaluation/run-evals/component-level-evals), or [end-to-end](/docs/llm-evaluation/run-evals/end-to-end-evals) should you wish to treat your LLM app as a black-box.
</Callout>

</details>

## Feature Highlights

<Cards>
  <Cards.Card arrow title="Metrics" href="/llm-evaluation/metrics/create-locally" />
  <Cards.Card
    arrow
    title="Running Evals"
    href="/llm-evaluation/running-llm-evals"
  />
  <Cards.Card
    arrow
    title="Unit-Testing in CI/CD"
    href="/llm-evaluation/evaluation-features/unit-testing-in-cicd-in-cicd"
  />
  <Cards.Card
    arrow
    title="Testing Reports"
    href="/llm-evaluation/evaluation-features/testing-reports"
  />
  <Cards.Card
    arrow
    title="A|B Regression Testing"
    href="/llm-evaluation/evaluation-features/ab-regression-testing"
  />
  <Cards.Card arrow title="Parameter Insights" href="/llm-evaluation/evaluation-features/parameter-insights" />
</Cards>

## Quickstart

This walkthrough will show how to run LLM evaluations on Confident AI. Not all features will be covered, but if you follow all the steps youâ€™ll have an ideal LLM evaluation pipeline setup.

### Create Metric

Import metrics import DeepEval. In this example we're using `GEval` to create a custom answer relevancy metric:

```python filename="main.py"
from deepeval.test_case import LLMTestCaseParams
from deepeval.metrics import GEval

metric = GEval(
  name="Relevancy",
  criteria="Determine how relevant the 'actual output' is to the 'input'"
  evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)
```

<details>
  <summary>How to customize your LLM judge?</summary>

  99.99% of metrics on DeepEval uses LLM-as-a-judge, which means you'll have to set your evaluation model. For most users, this will be using OpenAI, and you'll need to set your OpenAI API key:

  ```bash
  export OPENAI_API_KEY="sk-..."
  ```

  You can also use other model providers that `deepeval` has integrations with, such as:

  - [Ollama](https://deepeval.com/integrations/models/ollama)
  - [Azure OpenAI](https://deepeval.com/integrations/models/azure-openai)
  - [Anthropic](https://deepeval.com/integrations/models/anthropic)
  - [Gemini](https://deepeval.com/integrations/models/gemini)
  - [Vertex AI](https://deepeval.com/integrations/models/vertex-ai)
  - [vLLM](https://deepeval.com/integrations/models/vllm)
  - [LMStudio](https://deepeval.com/integrations/models/lmstudio)

  Lastly, you can wrap your own LLM API in `deepeval`'s `DeepEvalBaseLLM` class to use **ANY** model of your choice. [Click here](https://deepeval.com/guides/guides-using-custom-llms) to learn how.

  <Callout type="info">
    This will run evaluations locally before sending results over to Confident AI.
    You can also run evals on the cloud by [creating a metric
    collection.](/docs/llm-evaluation/metrics/create-on-the-cloud)
  </Callout>
</details>

### Prepare for Evaluation

Decorate your LLM app (replace with your own):

```python filename="main.py" showLineNumbers {4}
from openai import OpenAI
from deepeval.tracing import observe

@observe()
def your_llm_app(query: str) -> str:
    return openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": query}
        ]
    ).choices[0].message["content"]
```

Pull the dataset you've created (full guide [here](/docs/dataset-editor/introduction)):

```python filename="main.py" showLineNumbers {5}
from deepeval.dataset import EvaluationDataset

# Pull your dataset from Confident AI
dataset = EvaluationDataset()
dataset.pull(alias="your-dataset-alias")
```

### Run an LLM Eval

Putting everything together, run your first LLM evaluation:

```python filename="main.py" showLineNumbers {19, 24} copy
from openai import OpenAI
from deepeval.tracing import observe
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import evaluate
 
@observe()
def your_llm_app(query: str) -> str:
    return openai.ChatCompletion.create(model="gpt-4o", messages=[{"role": "user", "content": query}]
    ).choices[0].message["content"]

dataset = EvaluationDataset()
dataset.pull(alias="your-dataset-alias")

# Process each golden in your dataset
for goldens in dataset.goldens:
    input = golden.input
    test_case = LLMTestCase(input=input, actual_output=your_llm_app(input))
    dataset.test_cases.append(test_case)


# Run an evaluation
evaluate(test_cases=dataset.test_cases, metrics=[AnswerRelevancyMetric()])
```

Congratulations ðŸŽ‰! Your test run is now available on Confident AI automatically as a testing report.

### Identify Failing Test Case(s)

Identify your failing test cases in the testing report on Confident AI.

<Callout type="info">This testing report is also publicaly sharable.</Callout>

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:testing-report.mp4"
  width="100%"
  title="Identify Failing LLM Test Cases"
/>

## Future Roadmap

- [x] Editor table columns for custom metrics
- [x] Better hyperparameters display
- [x] Insights page
- [ ] Full text-search on test cases
