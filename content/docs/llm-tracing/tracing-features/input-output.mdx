---
title: Input/Output
---

import { Callout, Tabs } from "nextra/components";

# Input/Output

Both traces and spans have inputs and outputs, which you can set dynamically within your application using the `update_current_span` and `update_current_trace` function respectively.

## Set Trace I/O At Runtime

You can set the `input` and `output` of a trace using the `update_current_trace` function:

<Tabs items={['Python', 'JS/TypeScript']}>
  <Tabs.Tab>
    ```python filename="main.py" showLineNumbers {15}
    from openai import OpenAI
    from deepeval.tracing import observe, update_current_trace

    client = OpenAI()
     
    @observe()
    def llm_app(query: str):
        res = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": query}
            ]
        ).choices[0].message.content
    
        update_current_trace(input=query, output=res)
        return res
     
    llm_app("Write me a poem.")
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```typescript filename="index.ts" showLineNumbers {13}
    import OpenAI from 'openai';
    import { observe, updateCurrentTrace } from '@deepeval-ts/tracing';
    
    const openai = new OpenAI();
    
    const llmApp = (query: string): string => {
      const res = openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          { role: "user", content: query }
        ]
      }).choices[0].message.content;
      updateCurrentTrace({ input: query, output: res });
      return res;
    };
    
    const observedLlmApp = observe({
      fn: llmApp
    });
    
    observedLlmApp("Write me a poem.");
    ```
  </Tabs.Tab>
</Tabs>

The `input` and `output` can be **ANY TYPE**, and is useful for visualization on the UI (even more so if you're using [conversation threads](/docs/llm-tracing/tracing-features/threads)).

### Big disclaimer for threads

**HOWEVER**, if you are planning to [create a thread](/docs/llm-tracing/tracing-features/threads) from the traces (for chatbots, conversations, etc. multi-turn use cases), it is highly recommended that you provide **strings** instead, where the `input` will represent the user input, and `output` representing the LLM generated output. You can also leave out any `input` or `output` for consecutive user/LLM behaviors. 

You will also need the `input` and `output` to run [online evaluations on a thread](/docs/llm-tracing/online-evaluations#online-evals-for-threads), as these will be used as the turns for a conversational test case.

## Set Span I/O At Runtime


You can set the `input` and `output` on spans using the `update_current_span` function.

One thing to note however, is we recommend [setting attributes](/docs/llm-tracing/tracing-features/attributes) instead of `input` and `output` directly on spans that are one of the **DEFAULT span types**, since DeepEval handles the `input` and `output` setting automatically for default span `type`s. 

<Callout type="warning">
For example, the `"retriever"` span `type` expects a string as the `input` and list of strings as the `output`, which you might violate if setting I/O yourself. This will decrease the chances that you run into an error.
</Callout>

<Tabs items={['Python', 'JS/TypeScript']}>
  <Tabs.Tab>
    ```python filename="main.py" showLineNumbers {15}
    from openai import OpenAI
    from deepeval.tracing import observe, update_current_span

    client = OpenAI()
     
    @observe()
    def llm_app(query: str):
        res = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": query}
            ]
        ).choices[0].message.content
    
        update_current_span(input=query, output=res)
        return res
     
    llm_app("Write me a poem.")
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```typescript filename="index.ts" showLineNumbers {13}
    import OpenAI from 'openai';
    import { observe, updateCurrentSpan } from '@deepeval-ts/tracing';
    
    const openai = new OpenAI();
    
    const llmApp = (query: string): string => {
      const res = openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          { role: "user", content: query }
        ]
      }).choices[0].message.content;
      updateCurrentSpan({ input: query, output: res });
      return res;
    };
    
    const observedLlmApp = observe({
      fn: llmApp
    });
    
    observedLlmApp("Write me a poem.");
    ```
  </Tabs.Tab>
</Tabs>

This example is the same as the one for tracing except for the `update_current_trace`, and that's not a mistake. You can set `input` and `output`s the same way as you do for traces, and if a trace's I/O is not set it defaults to the I/O of the root span.

The `input` and `output` can be **ANY TYPE** for custom span `type`s, and is useful for visualization on the UI.
