---
title: Placeholder
---

import { Callout, Cards } from "nextra/components";

# LLM Tracing on Confident AI


Confident AI offers an **Observatory** for teams to do tracing and evals in one place. Think Datadog with evals for LLM apps.

<Callout>
When you use tracing on Confident AI, all traces, spans, and threads **automatically have access to all the evaluation DeepEval offers.**
</Callout>

Tracing is designed to be completely non-intrusive to your application. It:

- Requires no rewrite of your existing code - just add the `@observe` decorator
- Runs asynchronously in the background with zero impact on latency
- Fails silently if there are any issues, ensuring your app keeps running
- Works with any function signature - you can set input/output at runtime

## Feature Highlights

<Cards>
  <Cards.Card arrow title="Extensive LLM Tracing" href="/llm-observability/llm-tracing" />
  <Cards.Card
    arrow
    title="Online Evaluation"
    href="/llm-observability/functionalities/online-metrics"
  />
  <Cards.Card
    arrow
    title="Latency and Cost Tracking"
    href="/llm-observability/latency-and-cost-tracking"
  />
  <Cards.Card
    arrow
    title="Filters and Searches"
    href="/llm-observability/filters-and-searches"
  />
  <Cards.Card
    arrow
    title="Performance Alerting"
    href="/llm-observability/performance-alerting"
  />
</Cards>

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/observability:llm-tracing.mp4"
  width="100%"
  title="LLM Tracing for an Agentic RAG App"
/>

## Quick Walkthrough

This walkthrough will show how to trace LLM applications on Confident AI. Not all features will be covered, but if you follow all the steps you'll have LLM tracing setup.

### Setup Tracing

Here's a simple tracing implementation that monitors an OpenAI generation using the `@observe` decorator:

```python showLineNumbers
from deepeval.tracing import observe, update_current_span


@observe(type="llm", model="gpt-4")
def generate_response(prompt: str) -> str:
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    output = response.choices[0].message.content

    update_current_span(
      attributes=LlmAttributes(input=prompt, output=output)
    )
    return output
```

### Enable Online Metrics (recommended)

This step is optional but, you can enable online metrics to evaluate all traces logged on Confident AI. First, enable online metrics on Confident AI by goin got **Metrics** > **Collections**, and create a new metric collection. Then add the "Answer Relevancy" metric to this newly created collection, and make sure it is activated. Finally, select the metric collection, and click **Enable for monitoring**.

Now in your code, add these lines to automatically run online evals in production:

```python showLineNumbers {10, 19}
from openai import OpenAI
from deepeval.test_case import LLMTestCase
from deepeval.tracing import (
    observe,
    update_current_span,
    LlmAttributes,
)

@observe(type="llm", model="gpt-4", metrics="Answer Relevancy")
def generate_response(prompt: str) -> str:
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "user", "content": prompt}]
    )
    output = response.choices[0].message.content

    update_current_span(
      attributes=LlmAttributes(input=prompt, output=output)
      test_case=LLMTestCase(input=prompt, actual_output=output)
    )
    return output
```

Now whenever you run your `generate_response()` function, all traces will be logged and evaluated on Confident AI.

### Trace a Generation

Once you have everything setup, all you have to do is run the `generate_response()` function:

```python
...

generate_response("Hi!")
```
