---
title: LlamaIndex
---

import { Callout } from "nextra/components";

# LlamaIndex
[LlamaIndex](https://www.llamaindex.ai/) is a framework that makes it easy to build production agents that can find information, synthesize insights, generate reports, and take actions over the most complex enterprise data.

## Quickstart
Confident AI provides a `LlamaIndexEventHandler` that can be used to trace LlamaIndex events.

<Callout type="info">
[Event](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation/) in LlamaIndex represents a single moment in time that a certain occurrence took place within the execution of the applicationâ€™s code.
</Callout>


### Setup
Install the following packages:
```bash
pip install -U deepeval llama-index
```

### Create a sample agent
The agent will be able to perform calculations using the `multiply` tool.

```python showLineNumbers filename="main.py" {15, 18, 30}
import os
import asyncio
import time

from llama_index.core.agent.workflow import FunctionAgent
from llama_index.llms.openai import OpenAI
import llama_index.core.instrumentation as instrument

import deepeval
from deepeval.integrations import LLamaIndexEventHandler
from deepeval.tracing import observe

os.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"

deepeval.login_with_confident_api_key("<your-confident-api-key>")

dispatcher = instrument.get_dispatcher()
dispatcher.add_event_handler(LLamaIndexEventHandler())

def multiply(a: float, b: float) -> float:
    """Useful for multiplying two numbers."""
    return a * b

agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
)

@observe()
async def main():   
    response = await agent.run(
        "What's 7 * 8?"
    )
    print(response)

if __name__ == "__main__":
    asyncio.run(main())
    time.sleep(12)
```

The response will be something like this: `7 * 8 = 56`

### Viewing the traces
You can directly view the traces in the **Observatory** by clicking on the link in the output. The output after running the `main.py` file will be something like this:

```bash {4}
ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ 
7 * 8 equals 56.
[Confident AI Trace Log]  Successfully posted trace (0 traces remaining in queue, 1 in flight): 
https://app.confident-ai.com/project/<your-project-id>/observatory/traces/<your-trace-id> 
To disable dev logging, set CONFIDENT_TRACE_VERBOSE=NO as an environment variable.
```

<Callout>
As the agent is running, it will export **events** to the `dispatcher`. The `dispatcher` will send the events to the `LlamaIndexEventHandler` which will convert the events to the appropiate **spans**, create a **trace** and send it to the [Observatory](https://documentation.confident-ai.com/llm-tracing/introduction).
</Callout>
