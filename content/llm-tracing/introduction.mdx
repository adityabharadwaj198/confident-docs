---
title: Introduction
---

import { Callout, Cards, Tabs } from "nextra/components";

# Introduction to LLM Tracing on Confident AI 

Confident AI offers an **Observatory** for teams to trace and monitor LLM applications. Think Datadog for LLM apps.

<Callout>
Tracing on Confident AI contains [all the features](/llm-tracing/tracing-features/attributes) you would expect but also enables DeepEval metrics to run on individual components.
</Callout>

Confident AI tracing is designed to be completely non-intrusive to your application. It:

- Requires no rewrite of your existing code - just add the `@observe` decorator
- Runs asynchronously in the background with zero impact on latency
- Fails silently if there are any issues, ensuring your app keeps running
- Works with any function signature - you can set input/output at runtime
- Allows you to [run online evaluations](/llm-tracing/online-evaluations) for spans and traces.

## Feature Highlights

<Cards>
  <Cards.Card arrow title="LLM Tracing Features" href="/llm-tracing/introduction" />
  <Cards.Card
    arrow
    title="Online Evaluations"
    href="/llm-tracing/online-evaluations"
  />
  <Cards.Card
    arrow
    title="Conversation Monitoring"
    href="/llm-tracing/online-evaluations"
  />
  <Cards.Card
    arrow
    title="Latency and Cost Tracking"
    href="/llm-tracing/latency-and-cost-tracking"
  />
  <Cards.Card
    arrow
    title="Filters and Searches"
    href="/llm-tracing/filters-and-searches"
  />
  <Cards.Card
    arrow
    title="Performance Alerting"
    href="/llm-tracing/performance-alerting"
  />
</Cards>

<VideoDisplayer
  src="https://confident-docs.s3.us-east-1.amazonaws.com/observability:llm-tracing.mp4"
  width="100%"
  title="LLM Tracing for an Agentic RAG App"
/>

## Quickstart

Two important terminologies to be aware of:

- **Trace**: The overall process of tracking and visualizing the execution flow of your LLM application
- **Span**: Individual units of work within your application (e.g., LLM calls, tool executions, retrievals)

Each observed function **CREATES A SPAN**, and **MANY SPANS MAKE UP A TRACE**. When you have tracing setup, you can run evaluations on both the trace and span level.

### Installation

Install DeepEval and setup your tracing enviornment:

<Tabs items={['Python', 'JS/TypeScript']}>
  <Tabs.Tab>
```bash
pip install deepeval
```

Don't forget to login using your API key on Confident AI in the CLI:

```bash
deepeval login --confident-api-key YOUR_API_KEY
```

> [!NOTE]
>
> You can also login by using the `login_with_confident_api_key` method without going through the CLI if you're in a notebook environment:
>
> ```python
> import deepeval
>
> deepeval.login_with_confident_api_key("YOUR_API_KEY")
> ```

  </Tabs.Tab>
  <Tabs.Tab>

Using `npm`:
```bash
npm install deepeval-ts
```

Using `yarn`:
```bash
yarn add deepeval-ts
```

Don't forget to login with your API key:

```bash
export CONFIDENT_API_KEY=YOUR_API_KEY
```

> [!NOTE]
> 
> You can also do it directly in code:
> ```typescript
> import { traceManager } from '@deepeval-ts/tracing';
>
> traceManager.configure({
>	  confidentApiKey: "YOUR_API_KEY"
> })
> ```

  </Tabs.Tab>
</Tabs>

### Setup tracing

<Tabs items={['Python', 'JS/TypeScript']}>
  <Tabs.Tab>

The `@observe` decorator is the primary way to instrument your LLM application for tracing.

```python filename="main.py" showLineNumbers {4, 15}
from openai import OpenAI
from deepeval.tracing import observe

@observe()
def llm_app(query: str) -> str:
    return openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": query}
        ]
    ).choices[0].message["content"]


# Call app to send trace to Confident AI
llm_app("Write me a poem.")
```

If your `llm_app` has more than one function, simply decorate those functions with `@observe` too.

  </Tabs.Tab>
<Tabs.Tab>

The `observe` wrapper is the primary way to instrument your LLM application for tracing.

```typescript filename="index.ts" showLineNumbers {14, 20}
import OpenAI from 'openai';
import { observe } from '@deepeval-ts/tracing';
 
const openai = new OpenAI();
const llmApp = (query: string) => {
  return openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "user", content: query }
    ]
  }).choices[0].message.content;
};
 
const observedLlmApp = observe({
  fn: llmApp
});


// Call app to send trace to Confident AI
observedLlmApp("Write me a poem.")
```

If your `llmApp` has more than one function, simply wrap those functions in `observe` too and call the wrapped functions in `llmApp` instead.

  </Tabs.Tab>
</Tabs>


âœ… You can now go to the **Observatory** to see your traces there.

<Callout>
In a later section, you'll learn how to create [spans that are LLM specific](/llm-tracing/tracing-features/span-types#llm-span), which would allow you to log things such as token cost and model name automatically.
</Callout>

### Enable Online Evals 

You can enable online evaluation to run both [end-to-end](/llm-evaluation/run-evals/end-to-end-evals) (metrics on the trace) and [component-level](/llm-evaluation/run-evals/component-level-evals) (metrics on the span) evaluations on Confident AI. 

First, enable online evals on Confident AI by [creating a metric collection](/llm-evaluation/metrics/create-on-the-cloud#create-a-metric-collection), and adding at least one referenceless metric to it. Now in your code, add these lines to automatically run online evals in production:

<Tabs items={['Python', 'JS/TypeScript']}>
  <Tabs.Tab>

```python filename="main.py" showLineNumbers {4, 13}
from openai import OpenAI
from deepeval.tracing import observe, update_current_span

@observe(metricCollection=["My Metrics"])
def llm_app(query: str) -> str:
    res = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": query}
        ]
    ).choices[0].message["content"]

    update_current_span(test_case=LLMTestCase(input=query, actual_output=res))
    return res


# Call app to send trace to Confident AI
llm_app("Write me a poem.")
```

  </Tabs.Tab>
  <Tabs.Tab>

The `observe` wrapper is the primary way to instrument your LLM application for tracing.

```typescript filename="index.ts" showLineNumbers {14, 20}
import OpenAI from 'openai';
import { observe, updateCurrentSpan } from '@deepeval-ts/tracing';
 
const openai = new OpenAI();
const llmApp = (query: string) => {
  const res = openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "user", content: query }
    ]
  }).choices[0].message.content;
  
  updateCurrentSpan(
    testCase=new LLMTestCase({input=query, actualOutput=res})
  )
  return res
};
 
const observedLlmApp = observe({
  metricCollection=["My Metrics"],
  fn: llmApp
});


// Call app to send trace to Confident AI
observedLlmApp("Write me a poem.")
```

  </Tabs.Tab>
</Tabs>

Congratulations ðŸŽ‰! Now whenever you run your LLM app, all traces will be logged **AND** evaluated on Confident AI.

<Callout type="info">
More information on online evaluations can be found [here.](/llm-tracing/online-evaluations)
</Callout>

## Why Tracing On Confident AI?

Confident AI is the only platform where you can leverage DeepEval's evaluations on traces. Confident AI is also feature complete for your LLM tracing expectations, with features including:

- [Environments](/llm-tracing/tracing-features/environment)
- [Conversation logging](/llm-tracing/tracing-features/threads)
- [PII masking](/llm-tracing/tracing-features/masking)
- [Setting custom metadata](/llm-tracing/tracing-features/metadata)
- [Rate sampling](/llm-tracing/tracing-features/sampling)
- [Tags](/llm-tracing/tracing-features/tags)
- etc.

You can run evaluations on individual spans (component-level), traces (end-to-end), or threads (conversation evals), with access to an unlimited eval use cases through DeepEval. Whatever you're using for evals in development, bring it to prod.