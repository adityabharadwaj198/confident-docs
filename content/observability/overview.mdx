---
title: Overview
---

import { Callout, Cards } from "nextra/components";

# Observability Overview

Confident AI offers an **Observatory** for teams to trace and monitor LLM applications. Think Datadog for LLM apps.

The observatory allows you to:

- Debug LLM components in production
- Filter and search for historical generations
- Leave feedback
- Run evals on it
- Cost and latency tracking

## Features Summary

<Cards>
  <Cards.Card arrow title="LLM Tracing" href="/observability/llm-tracing" />
  <Cards.Card
    arrow
    title="Online Metrics"
    href="/observability/online-metrics"
  />
  <Cards.Card
    arrow
    title="Latency and Cost Tracking"
    href="/observability/latency-and-cost-tracking"
  />
  <Cards.Card
    arrow
    title="Filters and Searches"
    href="/observability/filters-and-searches"
  />
  <Cards.Card
    arrow
    title="Performance Alerting"
    href="/observability/performance-alerting"
  />
</Cards>

## Simple Walkthrough

### Setup Tracing

### Enable Online Metrics

### Trace a Generation

#### Logging your prompt

### Track Latency & Cost

### Filter for Traces

## Future Roadmap

- [ ] Hyperparameter logging
- [ ] Custom properties logging
- [ ] Latency and cost tracking display
- [ ] Self-served alerting notification configurations
- [ ] More integrations
- [ ] Span level metrics defining
